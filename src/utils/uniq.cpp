/* uniq: remove duplicate reads from a file of mapped reads in the
 * dnmtools format (as output from format_reads), based on identical
 * mapping location and alignment to the reference.
 *
 * Copyright (C) 2013-2023 University of Southern California and
 *                         Andrew D. Smith
 *
 * Author: Andrew D. Smith
 *
 * This program is free software: you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation, either version 3 of the
 * License, or (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */

#include <cstdint>  // for [u]int[0-9]+_t
#include <iostream>
#include <random>
#include <stdexcept>
#include <string>
#include <vector>

// generated by autotools
#include <config.h>

#include "GenomicRegion.hpp"
#include "OptionParser.hpp"
#include "bsutils.hpp"
#include "dnmt_error.hpp"
#include "smithlab_os.hpp"
#include "smithlab_utils.hpp"
#include "bam_record_utils.hpp"

using std::cerr;
using std::endl;
using std::ifstream;
using std::ofstream;
using std::runtime_error;
using std::string;
using std::to_string;
using std::vector;

using bamxx::bam_rec;

namespace uniq_random {
  // ADS: I made this namespace and functions because different
  // implementations of rand() on different OS meant that even with
  // the same seed, the results could be different. This meant testing
  // didn't work.
  // ADS: (TODO) refactor this
  bool initialized = false;
  std::default_random_engine e;
  std::uniform_int_distribution<int> di;
  void initialize(const size_t the_seed) {
    e = std::default_random_engine(the_seed);
    initialized = true;
  }
  int rand() {
    // ADS: should have same range as ordinary rand() by properties of
    // std::uniform_int_distribution default constructor.
    // assert(initialized);
    return di(e);
  }
}  // namespace uniq_random

struct rd_stats {  // keep track of good bases/reads in and out
  size_t bases{};
  size_t reads{};
  void update(const bam_rec &b) {
    bases += get_l_qseq(b);
    ++reads;
  }
};


struct uniq_summary {
  uniq_summary(const rd_stats &rs_in, const rd_stats &rs_out,
               const size_t reads_duped) {
    total_reads = rs_in.reads;
    total_bases = rs_in.bases;
    unique_reads = rs_out.reads;
    unique_read_bases = rs_out.bases;
    reads_removed = rs_in.reads - rs_out.reads;
    non_duplicate_fraction = static_cast<double>(rs_out.reads - reads_duped) /
                             std::max(1ul, rs_in.reads);
    duplication_rate = static_cast<double>(reads_removed + reads_duped) /
                       std::max(1ul, reads_duped);
    duplicate_reads = reads_duped;
  }

  // total_reads is the number of input reads
  size_t total_reads{};
  // total_bases is the total number of input bases
  size_t total_bases{};
  // unique_reads is the number of unique reads
  size_t unique_reads{};
  // unique_read_bases is the total number of bases for the unique reads
  size_t unique_read_bases{};
  // non_duplicate_fraction is the ratio of the number of unique reads with
  // no duplicates to that of the input reads
  double non_duplicate_fraction{};
  // duplicate_reads is the number of unique reads with at least one duplicate
  size_t duplicate_reads{};
  // reads_removed is the number of duplicate reads that have been removed
  size_t reads_removed{};
  // duplication_rate is the average number of duplicates for the reads with
  // at least one duplicate (>1 by definition)
  double duplication_rate{};

  string tostring() {
    std::ostringstream oss;
    oss << "total_reads: " << total_reads << endl
        << "total_bases: " << total_bases << endl
        << "unique_reads: " << unique_reads << endl
        << "unique_read_bases: " << unique_read_bases << endl
        << "non_duplicate_fraction: " << non_duplicate_fraction << endl
        << "duplicate_reads: " << duplicate_reads << endl
        << "reads_removed: " << reads_removed << endl
        << "duplication_rate: " << duplication_rate;

    return oss.str();
  }
};



static void
write_stats_output(const rd_stats &rs_in, const rd_stats &rs_out,
                   const size_t reads_duped, const string &statfile) {
  if (!statfile.empty()) {
    uniq_summary summary(rs_in, rs_out, reads_duped);
    ofstream out_stat(statfile);
    if (!out_stat) throw runtime_error("bad stats output file");
    out_stat << summary.tostring() << endl;
  }
}

static void
write_hist_output(const vector<size_t> &hist, const string &histfile) {
  if (!histfile.empty()) {
    ofstream out_hist(histfile);
    if (!out_hist) throw runtime_error("bad hist output file");
    for (size_t i = 0; i < hist.size(); ++i)
      if (hist[i] > 0) out_hist << i << '\t' << hist[i] << '\n';
  }
}

/* The "inner" buffer corresponds to all reads sharing chrom, start,
   end and strand, and is a contiguous subset of the "outer" buffer
   that shares the same end and strand. */
static void
process_inner_buffer(const bool add_dup_count,
                     const vector<bam_rec>::iterator it,
                     const vector<bam_rec>::iterator jt, bamxx::bam_header &hdr,
                     bamxx::bam_out &out, rd_stats &rs_out, size_t &reads_duped,
                     vector<size_t> &hist) {
  constexpr char du_tag[2] = {'D', 'U'};
  const size_t n_reads = std::distance(it, jt);
  const size_t selected = uniq_random::rand() % n_reads;

  if (add_dup_count) {
    const int ret = bam_aux_update_int(*(it + selected), du_tag, n_reads);
    if (ret < 0) throw dnmt_error("error adding duplicate count aux field");
  }

  if (!out.write(hdr, *(it + selected)))
    throw runtime_error("failed writing bam record");
  if (hist.size() <= n_reads) hist.resize(n_reads + 1);
  hist[n_reads]++;
  rs_out.update(*(it + selected));
  reads_duped += (n_reads > 1);
}

/* The buffer corresponds to reads sharing the same mapping chromosome
   and start position. These are gathered and then processed together. */
static void
process_buffer(const bool add_dup_count, rd_stats &rs_out, size_t &reads_duped,
               vector<size_t> &hist, vector<bam_rec> &buffer, bamxx::bam_header &hdr,
               bamxx::bam_out &out) {
  sort(begin(buffer), end(buffer), precedes_by_end_and_strand);
  auto it(begin(buffer));
  auto jt = it + 1;
  for (; jt != end(buffer); ++jt)
    if (!equivalent_end_and_strand(*it, *jt)) {
      process_inner_buffer(add_dup_count, it, jt, hdr, out, rs_out, reads_duped,
                           hist);
      it = jt;
    }
  process_inner_buffer(add_dup_count, it, jt, hdr, out, rs_out, reads_duped,
                       hist);
  buffer.clear();
}

static void
uniq(const bool add_dup_count, const uint32_t max_buffer_size,
     const size_t n_threads, const string &cmd, const string &infile,
     const string &statfile, const string &histfile, const bool bam_format,
     const string &outfile) {
  // values to tabulate stats; no real cost
  rd_stats rs_in, rs_out;
  size_t reads_duped = 0;
  vector<size_t> hist;

  bamxx::bam_tpool tpool(n_threads);  // outer scope: must be destroyed last

  bamxx::bam_in hts(infile);
  if (!hts) throw dnmt_error("failed to open input file: " + infile);
  bamxx::bam_header hdr(hts);
  if (!hdr) throw dnmt_error("failed to read header");

  bamxx::bam_out out(outfile, bam_format);
  {
    bamxx::bam_header hdr_out(hdr);
    if (!hdr_out) throw dnmt_error("failed create header");
    hdr_out.add_pg_line(cmd, "DNMTOOLS", VERSION);
    if (!out.write(hdr_out)) throw dnmt_error("failed to write header");
  }

  if (n_threads > 1) {
    tpool.set_io(hts);
    tpool.set_io(out);
  }

  bam_rec aln;
  if (hts.read(hdr, aln)) {  // valid SAM/BAM can have 0 reads

    rs_in.update(aln);  // update stats for input we just got

    vector<bam_rec> buffer(1, aln);  // select output from this buffer

    // to check that reads are sorted properly
    vector<bool> chroms_seen(get_n_targets(hdr), false);
    int32_t cur_chrom = get_tid(aln);

    while (hts.read(hdr, aln)) {
      rs_in.update(aln);

      // below works because buffer reset at every new chrom
      if (precedes_by_start(aln, buffer[0]))
        throw runtime_error("not sorted: " + get_qname(buffer[0]) + " " +
                            get_qname(aln));

      const int32_t chrom = get_tid(aln);
      if (chrom != cur_chrom) {
        if (chroms_seen[chrom]) throw runtime_error("input not sorted");
        chroms_seen[chrom] = true;
        cur_chrom = chrom;
      }

      if (!equivalent_chrom_and_start(buffer[0], aln))
        process_buffer(add_dup_count, rs_out, reads_duped, hist, buffer, hdr,
                       out);
      if (size(buffer) < max_buffer_size || add_dup_count)
        buffer.push_back(aln);
      else if (!add_dup_count)
        std::swap(buffer[uniq_random::rand() % max_buffer_size], aln);
    }
    process_buffer(add_dup_count, rs_out, reads_duped, hist, buffer, hdr, out);
  }
  // write any additional output requested
  write_stats_output(rs_in, rs_out, reads_duped, statfile);
  write_hist_output(hist, histfile);
}

int
main_uniq(int argc, const char **argv) {
  try {
    uint32_t max_buffer_size = std::numeric_limits<uint32_t>::max();
    bool VERBOSE = false;

    bool bam_format = false;
    bool add_dup_count = false;
    bool use_stdout = false;

    // ADS: Not recommended to change this seed. It shouldn't matter
    // at all, and we want results to behave as deterministic.
    size_t the_seed = 408;
    string outfile;
    string statfile;
    string histfile;
    size_t n_threads = 1;

    /****************** COMMAND LINE OPTIONS ********************/
    OptionParser opt_parse(strip_path(argv[0]),
                           "program to remove duplicate reads from "
                           "sorted mapped reads",
                           "<in-file> [out-file]", 2);
    opt_parse.add_opt("threads", 't', "number of threads", false, n_threads);
    opt_parse.add_opt("summary", 'S', "statistics output file", false, statfile);
    opt_parse.add_opt("add-count", 'a', "add duplicate counts to reads", false,
                      add_dup_count);
    opt_parse.add_opt("hist", '\0',
                      "histogram output file for library"
                      " complexity analysis",
                      false, histfile);
    opt_parse.add_opt("bam", 'B', "output in BAM format", false, bam_format);
    opt_parse.add_opt("stdout", '\0', "write to standard output", false,
                      use_stdout);
    opt_parse.add_opt("seed", 's', "random seed", false, the_seed);
    opt_parse.add_opt("max", 'm', "max duplicates to consider",
                      false, max_buffer_size);
    opt_parse.add_opt("verbose", 'v', "print more run info", false, VERBOSE);
    opt_parse.set_show_defaults();
    vector<string> leftover_args;
    opt_parse.parse(argc, argv, leftover_args);
    if (opt_parse.about_requested() || opt_parse.help_requested() ||
        leftover_args.empty()) {
      cerr << opt_parse.help_message() << endl
           << opt_parse.about_message() << endl;
      return EXIT_SUCCESS;
    }
    if (opt_parse.option_missing()) {
      cerr << opt_parse.option_missing_message() << endl;
      return EXIT_SUCCESS;
    }
    if ((leftover_args.size() == 1 && !use_stdout) ||
        (leftover_args.size() == 2 && use_stdout)) {
      cerr << opt_parse.help_message() << endl
           << opt_parse.about_message() << endl;
      return EXIT_SUCCESS;
    }
    const string infile(leftover_args.front());
    if (leftover_args.size() == 2 && !use_stdout)
      outfile = leftover_args.back();
    else
      outfile = string("-");  // so htslib can write to stdout
    /****************** END COMMAND LINE OPTIONS *****************/

    // ADS: Random here is because we choose randomly when keeping one
    // among duplicate reads.
    uniq_random::initialize(the_seed);

    std::ostringstream cmd;
    copy(argv, argv + argc, std::ostream_iterator<const char *>(cmd, " "));

    if (VERBOSE)
      cerr << "[output file: " << outfile << "]" << endl
           << "[output format: " << (bam_format ? "B" : "S") << "AM]" << endl
           << "[stats file: " << (statfile.empty() ? "none" : statfile) << "]"
           << endl
           << "[hist file: " << (histfile.empty() ? "none" : histfile) << "]"
           << endl
           << "[add duplicate count: " << (add_dup_count ? "yes" : "no") << "]"
           << endl
           << "[threads requested: " << n_threads << "]" << endl
           << "[command line: \"" << cmd.str() << "\"]" << endl
           << "[random number seed: " << the_seed << "]" << endl;

    uniq(add_dup_count, max_buffer_size, n_threads, cmd.str(), infile, statfile,
         histfile, bam_format, outfile);
  }
  catch (const runtime_error &e) {
    cerr << e.what() << endl;
    return EXIT_FAILURE;
  }
  return EXIT_SUCCESS;
}
